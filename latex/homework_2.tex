\documentclass[11pt]{article}

\input{./preamble.tex}

%%
%% DOCUMENT START
%%

\begin{document}


\newcommand{\widesim}[2][1.5]{
  \mathrel{\overset{#2}{\scalebox{#1}[1]{$\sim$}}}
}

\pagestyle{fancyplain}
\lhead{}
\chead{}
\rhead{}
\lfoot{\hrule UQ: Homework 2}
\cfoot{\hrule \thepage}
\rfoot{\hrule Ryan Skinner}

\noindent
{\Large Homework 2}
\hfill
{\large Ryan Skinner}
\\[0.5ex]
{\large ASEN 6519: Uncertainty Quantification}
\hfill
{\large Due 2016/03/08}\\
\hrule
\vspace{6pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 1} %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Given the joint CDF of random variables $X_1$ and $X_2$,
\begin{equation}
F_{X_1,X_2}(x_1,x_2) = 1 - \exp(-x_1) - \exp(-x_2) + \exp(-x_1-x_2-x_1x_2)
, \quad x_1,x_2 \ge 0,
\end{equation}
we are tasked with finding the marginal CDF $F_{X_1}(x_1)$ and the conditional CDF $F_{X_2|X_1}(x_2|x_1)$, and subsequently generating realizations of $X_1,X_2$ using the inversion method.

The marginal CDF of $X_1$ is trivially calculated in the limit $x_2 \rightarrow \infty$ as
\begin{equation}
\boxed{F_{X_1}(x_1)} = F_{X_1,X_2}(x_1,\infty) = 1 - \exp(-x_1)
.
\end{equation}

Applying the relation
\begin{equation}
F_{X_2|X_1}(x_2|x_1) = \left( \int_0^{x_2} f_{X_1,X_2}(x_1,t_2) dt_2 \right) \Big/ f_{X_1}(x_1)
,
\end{equation}
where the marginal and joint pdfs are
\begin{align*}
f_{X_1}(x_1) &= \partial_{x_1} F_{X_1}(x_1) \\
f_{X_1,X_2}(x_1,x_2) &= \partial_{x_1} \partial_{x_2} F_{X_1,X_2}(x_1,x_2)
,
\end{align*}
it can be shown that
\begin{equation}
\boxed{F_{X_2|X_1}(x_2|x_1)} = 1 - (1+x_2) \exp(-[1+x_1]x_2)
,
\end{equation}
which is impossible to invert analytically, though computational root-finding methods show success.

Realizations of $X_1,X_2$ are generated in the standard manner: for each $i = 1, ..., N$, a random variable $U_1^i \sim U[0,1]$ is generated, and set equal to $F_{X_1}(x_1)$, which can be solved for realization $x_1^i$. Another random variable $U_2^i \sim U[0,1]$ is generated and set equal to $F_{X_2|X_1}(x_2|x_1)$, which is then solved numerically for $x_2^i$, given $x_1^i$. We choose Matlab's `fzero` function as our root finder.

In \figref{fig:prob1}, we generate $N=10,\!000$ realizations and compare the cumulative expectation of the first $n$ samples to the analytical expectations
\begin{equation}
\begin{aligned}
\langle x_1 \rangle &= 1.0 \\
\langle x_1 \rangle &= 1.0 \\
\langle x_1 x_2 \rangle &= 0.596347
\end{aligned}
\end{equation}
All three quantities approach their analytical values, and the relative error in each quantity is seen to decrease as $n$ increases.

This method of verification is by no means rigorous. The mean square error of the empirical CDF or pdf would be a better way of checking the validity of our answers, but this suffices for the purposes of this exercise.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth, trim={1in 0 1in 0}]{Prob1.eps}
\caption{Expectation values of various functions of $x_1$ and $x_2$, using the first $n$ cumulative samples. Analytical expectation values plotted to show convergence, as well as relative error.}
\label{fig:prob1}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 2} %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This problem concerns a derivation from scratch of the Bayesian MAP estimate of a random variable $V$, assuming a Gaussian prior $V \sim N(V_0, \sigma_0^2)$. Further details are worked by hand on the attached sheets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 3} %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The thermal coefficient $K$ of a 1D slab is characterized by a lognormal random process
\begin{equation*}
K(x,\omega) = \exp(G(x,\omega)), \qquad x \in (0,1),
\end{equation*}
where $G(x, \omega)$ is a Gaussian random process defined on $(0, 1)$. The mean and covariance functions of the Gaussian process $G(x, \omega)$ are
\begin{equation*}
\xpect{G(x)} = 1.0, \qquad x \in (0,1),
\end{equation*}
and
\begin{equation*}
C_{GG}(x_1,x_2) = \sigma^2 \exp \left( \frac{- | x_1-x_2 | }{\ell} \right), \qquad (x1,x2) \in (0,1) \times (0,1),
\end{equation*}
respectively. We would like to compute the statistics of the temperature field $u(x)$ by solving the governing steady-sate stochastic heat equation
\begin{align}
\pp{}{x} \left( K(x,\omega) \pp{u(x,\omega)}{x} \right) &= 1.0, \qquad x \in (0,1), \\
u(0,\omega) &= 0, \\
u(1,\omega) &= \theta(\omega),
\label{eq:pde}
\end{align}
where $\theta(\omega) \sim N(0,0.1)$ characterizes the uncertainty in the right boundary condition and is statistically independent from $K(x,\omega)$. Set $\sigma=2$ and $\ell=0.2$, and use the codes form Homework 1 to generate samples of $G_d(x,\omega)$ when only $d=10$ terms in the KL expansion of $G(x,\omega)$ are used.

We write a standard second-order central-difference implicit code to solve the PDE in \eqref{eq:pde} for fixed $\omega$, meaning that $K(x,\omega) \rightarrow K(x)$ and $\theta(\omega) \rightarrow \theta$. Next, we compute the mean $\xpect{u(x)}$ and variance $\var(u(x))$ of the solution using a Monte Carlo simulation, and verify that these statistics converge as the number of samples is increased. Finally, letting $u_\tmax = \max_x u(x)$, we compute the probability that the maximum temperature on the slab exceeds a certain threshold,
\begin{equation}
P \left( u_\tmax > \xpect{u_\tmax} + 3 \cdot \sqrt{\var(u_\tmax)} \right)
.
\label{eq:probability}
\end{equation}

Using the analytical eigensystem solution for the Karhunen-Loeve expansion, realizations of $G$, $K$, and $u$ are shown in \figref{fig:prob3a}. A spatial discretization is chosen that uses 1001 uniformly-spaced nodes on the interval $[0,1]$. As the number of realizations $n$ is increased from 1 to $N=100,\!000$, the mean and variance converge to the functions presented in \figref{fig:prob3b}. Both boundary conditions are satisfied, since $u(0)=0$ is fixed and  $u(1) = \theta(\omega) \sim N(0,0.1)$ matches in both mean and variance plots. Finally, we see the probability converge to a value near 0.016 in \figref{fig:prob3c}. Though not shown in this figure, the final statistics pertaining to $u_\tmax$ obtained from 250,000 realizations are shown in Table \ref{tbl:probabilities}.

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
Quantity & Value \\
\midrule
$\xpect{u_\tmax}$ & 0.12568 \\
$\var(u_\tmax)$ & 0.03379 \\
$P(\cdot)$ & 1.59 \% \\
\bottomrule
\end{tabular}
\vspace{1em}
\caption{Relevant quantities in \eqref{eq:probability} computed from 250,000 realizations using 1,001 grid points.}
\label{tbl:probabilities}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.84\textwidth, trim={1cm 0 1cm 0}]{Prob3_a.eps}
\caption{Ten realizations of $G$, $K$, and $u$.}
\label{fig:prob3a}
\end{figure}

\begin{figure}[b]
\centering
\includegraphics[width=0.9\textwidth, trim={1cm 0 1cm 0}]{Prob3_b.eps}
\caption{Convergence of the mean and variance of $u(x)$ as the number $n$ of realizations is increased.}
\label{fig:prob3b}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth, trim={1cm 0 1cm 0}]{Prob3_c.eps}
\vspace{1em}
\caption{Convergence in the probability \eqref{eq:probability} as more realizations are generated. Final statistics use $2.5\times 10^5$ realizations, but only up to $10^4$ are shown here.}
\label{fig:prob3c}
\end{figure}

%%
%% DOCUMENT END
%%
\end{document}
